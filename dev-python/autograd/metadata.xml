<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="project">
		<email>sci@gentoo.org</email>
		<name>Gentoo Science Project</name>
	</maintainer>
	<maintainer type="person">
		<email>alexander@neuwirth-informatik.de</email>
		<name>Alexander Puck Neuwirth</name>
	</maintainer>
	<longdescription lang="en">
	Autograd can automatically differentiate native Python and Numpy code. It can handle a large subset of Python's features, including loops, ifs, recursion and closures, and it can even take derivatives of derivatives of derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation), which means it can efficiently take gradients of scalar-valued functions with respect to array-valued arguments, as well as forward-mode differentiation, and the two can be composed arbitrarily. The main intended application of Autograd is gradient-based optimization. 
	</longdescription>
	<upstream>
		<remote-id type="pypi">autograd</remote-id>
		<remote-id type="github">HIPS/autograd</remote-id>
	</upstream>
</pkgmetadata>
